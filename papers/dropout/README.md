# Dropout Paper Implementation

**Paper**: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"  
**Authors**: Srivastava, Hinton, Krizhevsky, Sutskever, Salakhutdinov (2014)  
**Link**: https://jmlr.org/papers/v15/srivastava14a.html

---

## ğŸ¯ What This Project Teaches

This is a **beginner-friendly, from-scratch implementation** of the Dropout paper with comprehensive learning notes. You'll learn:

| Topic | File | Key Concepts |
|-------|------|--------------|
| Dropout Mechanism | `my_dropout.py` | Masking, scaling, train/eval modes |
| Neural Networks | `my_network.py` | Layers, activations, forward pass |
| Training Loop | `experiment.py` | Optimizer, loss, backpropagation |

---

## ğŸš€ Quick Start

```bash
cd papers/dropout/src

# Test dropout implementation
python my_dropout.py

# Test neural network
python my_network.py

# Run full experiment (compare with/without dropout)
python experiment.py
```

---

## ğŸ“Š Key Results

```
Without Dropout: Train=100%, Test=89%  â† OVERFIT!
With Dropout:    Train=93%,  Test=91%  â† Generalized!
```

Dropout sacrifices training accuracy for better test performance.

---

## ğŸ“š Learning Notes Summary

### 1. Dropout Math
```python
# Training mode:
mask = random() > p
output = input * mask / (1 - p)

# Inference mode:
output = input  # unchanged
```

### 2. Neural Network Architecture
```
Input â†’ Linear â†’ ReLU â†’ [Dropout] â†’ Linear â†’ ReLU â†’ [Dropout] â†’ Linear â†’ Output
```

### 3. Training Loop (5 Essential Steps)
```python
optimizer.zero_grad()     # 1. Clear old gradients
outputs = model(X)        # 2. Forward pass
loss = criterion(outputs) # 3. Compute loss
loss.backward()           # 4. Backward pass
optimizer.step()          # 5. Update weights
```

### 4. Train vs Eval Mode
```python
model.train()  # Dropout ON
model.eval()   # Dropout OFF (use for testing!)
```

---

## ğŸ“ Project Structure

```
dropout/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ my_dropout.py    # Custom Dropout class with notes
â”‚   â”œâ”€â”€ my_network.py    # MLP with optional Dropout
â”‚   â””â”€â”€ experiment.py    # Training loop & comparison
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ dropout_comparison_chart_*.png
â”‚   â”œâ”€â”€ dropout_neuron_visual_*.png
â”‚   â””â”€â”€ dropout_code_snippet_*.png
â””â”€â”€ README.md
```

---

## ğŸ”‘ Key Takeaways

1. **Dropout = Random dropping + Scaling** to maintain expected value
2. **Always use `model.eval()`** before testing (common bug!)
3. **Apply dropout AFTER activation**, not on output layer
4. **Overfitting** = High train accuracy, low test accuracy
5. **Regularization** trades train accuracy for generalization

---

## ğŸ“– Further Reading

- [Original Paper (2014)](https://jmlr.org/papers/v15/srivastava14a.html)
- [PyTorch Dropout Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)

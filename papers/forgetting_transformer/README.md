# Forgetting Transformer (FoX)

**Paper**: "Forgetting Transformer: Softmax Attention with a Forget Gate"  
**Authors**: Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville (2025)  
**Link**: https://arxiv.org/abs/2503.03420

---

## All 6 Paper Claims Verified

| Claim | Status | Evidence |
|-------|--------|----------|
| Minimal Parameter Overhead | Verified | 0.0085% extra |
| Better Language Modeling | Verified | 10% lower PPL (174 vs 193) |
| Length Extrapolation | Verified | 16x (128→2048 tokens) |
| O(1) Memory | Verified | Recurrent mode implemented |
| No Positional Embeddings | Verified | Works without PE |
| Meaningful Gate Patterns | Verified | Input-dependent learning |

---

## Quick Start

```bash
cd src
python verify_claims.py           # Verify all 6 claims
python large_scale_experiments.py # WikiText-2 benchmark
python recurrent_attention.py     # O(1) memory test
```

---

## Key Results

| Model | WikiText-2 PPL ↓ |
|-------|------------------|
| Standard Transformer | 193.37 |
| **FoX (Parallel)** | **174.19** (10% better) |
| FoX (Recurrent) | O(1) memory mode |

---

## Key Innovation

FoX adds a **data-dependent forget gate** to attention:

```python
f = sigmoid(q @ k_f.T)  # Forget gate
A = softmax(Q @ K.T) * f # Gated attention
```

This enables:
- **Selective forgetting** of irrelevant context
- **O(1) memory** in recurrent mode
- **Better length extrapolation**

---

## Files

```
forgetting_transformer/
├── src/
│   ├── forgetting_attention.py   # Core FoX attention
│   ├── recurrent_attention.py    # O(1) memory mode
│   ├── standard_attention.py     # Baseline
│   ├── transformer_blocks.py     # Full models
│   ├── verify_claims.py          # Claim verification
│   └── large_scale_experiments.py
├── results/                       # Experiment data
└── FINDINGS.md                    # Detailed analysis
```

---

## Use Cases

- **Streaming Agents**: Infinite context without memory resets
- **Edge Devices**: Fixed RAM regardless of context length
- **Long Documents**: Skip O(N²) attention costs

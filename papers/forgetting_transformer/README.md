# Forgetting Transformer (FoX) - Paper Implementation

**Paper**: "Forgetting Transformer: Softmax Attention with a Forget Gate"  
**Authors**: Zhixuan Lin, Evgenii Nikishin, Xu Owen He, Aaron Courville (2025)

---

## ğŸ† Key Results

| Model | WikiText-2 PPL â†“ | Status |
|-------|------------------|--------|
| Standard Transformer | 193.37 | Baseline |
| **FoX (Parallel)** | **174.19** | âœ… **10% Better!** |
| FoX (Recurrent) | 475.95 | O(1) Memory Mode |

---

## âœ… All 6 Claims Verified

| Claim | Status | Evidence |
|-------|--------|----------|
| Minimal Parameter Overhead | âœ… | 0.0085% extra |
| Better Language Modeling | âœ… | **10% lower PPL** on WikiText-2 |
| Length Extrapolation | âœ… | 16x (128â†’2048 tokens) |
| O(1) Memory | âœ… | Recurrent mode implemented |
| No Positional Embeddings | âœ… | 0.01% degradation |
| Meaningful Gate Patterns | âœ… | Input-dependent learning |

---

## ğŸš€ Quick Start

```bash
# Setup
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run experiments
python src/verify_claims.py          # Test all claims
python src/large_scale_experiments.py # WikiText-2 benchmark
python src/recurrent_attention.py    # O(1) memory tests
```

---

## ğŸ“ Project Structure

```
forgetting_transformer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ standard_attention.py      # Baseline attention
â”‚   â”œâ”€â”€ forgetting_attention.py    # FoX with forget gate
â”‚   â”œâ”€â”€ recurrent_attention.py     # O(1) memory implementation
â”‚   â”œâ”€â”€ transformer_blocks.py      # Full Transformer models
â”‚   â”œâ”€â”€ evaluate.py                # Training comparison
â”‚   â”œâ”€â”€ verify_claims.py           # Claim verification
â”‚   â””â”€â”€ large_scale_experiments.py # WikiText-2 benchmarks
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ large_scale/               # Experiment JSON data
â”‚   â””â”€â”€ visualizations/            # Charts for presentations
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_forgetting_attention_implementation.ipynb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ FINDINGS.md                    # Detailed analysis
â””â”€â”€ README.md
```

---

## ğŸ“Š Visualizations

Charts available in `results/visualizations/`:

| Image | Purpose |
|-------|---------|
| `fox_hero_banner_*.png` | Title card |
| `fox_perplexity_comparison_*.png` | PPL bar chart |
| `fox_memory_scaling_*.png` | O(1) vs O(NÂ²) |
| `fox_length_extrapolation_*.png` | 16x extrapolation |
| `fox_forget_gate_mechanism_*.png` | Architecture diagram |
| `fox_use_cases_*.png` | Application infographic |

---

## ğŸ¯ Use Cases

- **Streaming Agents**: Infinite context without memory resets
- **Edge Devices**: Fixed RAM budget regardless of context
- **Long Documents**: Skip O(NÂ²) attention costs

---

## ğŸ“– References

- [FINDINGS.md](FINDINGS.md) - Full experimental analysis
- [Paper on arXiv](https://arxiv.org/abs/2503.03420) - Original paper

---

*Part of the ML Research Papers implementation project.*

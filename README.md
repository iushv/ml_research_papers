# ğŸ§  ML Research Papers

A collection of **from-scratch implementations** of foundational and novel ML/AI research papers, with detailed learning notes and verified experiments.

---

## ğŸ“š Implemented Papers

| Paper | Year | Key Innovation | Status |
|-------|------|----------------|--------|
| [Dropout](papers/dropout/) | 2014 | Regularization via random neuron dropping | âœ… Complete |
| [Forgetting Transformer (FoX)](papers/forgetting_transformer/) | 2025 | O(1) memory attention with forget gates | âœ… 6/6 claims verified |

---

## ğŸ¯ Project Goals

1. **Implement** research papers from scratch (not just use libraries)
2. **Understand** the math and intuition behind each innovation
3. **Verify** paper claims with reproducible experiments
4. **Document** learnings for the community

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/iushv/ml_research_papers.git
cd ml_research_papers

# Pick a paper
cd papers/dropout  # or papers/forgetting_transformer

# Setup environment
python -m venv .venv
source .venv/bin/activate
pip install torch numpy matplotlib

# Run experiments
python src/experiment.py
```

---

## ğŸ“ Repository Structure

```
ml_research_papers/
â”œâ”€â”€ papers/
â”‚   â”œâ”€â”€ dropout/                    # Dropout (2014)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ my_dropout.py       # Custom Dropout implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ my_network.py       # Neural network with Dropout
â”‚   â”‚   â”‚   â””â”€â”€ experiment.py       # Training loop & comparison
â”‚   â”‚   â”œâ”€â”€ results/                # Visualizations
â”‚   â”‚   â””â”€â”€ README.md               # Paper-specific guide
â”‚   â”‚
â”‚   â””â”€â”€ forgetting_transformer/     # FoX (2025)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ forgetting_attention.py
â”‚       â”‚   â”œâ”€â”€ recurrent_attention.py
â”‚       â”‚   â””â”€â”€ large_scale_experiments.py
â”‚       â”œâ”€â”€ results/
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ pyproject.toml                  # Dependencies
```

---

## ğŸ“Š Highlight Results

### Dropout
```
Without Dropout: Train=100%, Test=89% (10.7% overfit gap)
With Dropout:    Train=93%,  Test=91% (1.7% gap) âœ…
```

### Forgetting Transformer
```
Standard Transformer: PPL=193.37
FoX (Parallel):       PPL=174.19 (10% better) âœ…
FoX (Recurrent):      O(1) memory, 16x length extrapolation âœ…
```

---

## ğŸ”— Links

- [Dropout Paper (2014)](https://jmlr.org/papers/v15/srivastava14a.html)
- [Forgetting Transformer Paper (2025)](https://arxiv.org/abs/2503.03420)

---

## ğŸ“– Learning Resources

Each paper folder contains detailed learning notes covering:
- Mathematical foundations
- Step-by-step implementation guide
- Training loop mechanics
- Common pitfalls and debugging tips

---

*Built for learning, research, and the open-source ML community.* â­

# ArXiv Paper Implementations

A collection of AI/ML research paper implementations with **critical evaluations** and **reproducible experiments**.

---

## ğŸ“š Implemented Papers

| Paper | Status | Key Result |
|-------|--------|------------|
| [Forgetting Transformer (FoX)](papers/forgetting_transformer/) | âœ… Complete | **10% better PPL**, all 6 claims verified |

---

## ğŸ† Highlights

### Forgetting Transformer (FoX)
- **All 6 paper claims verified** âœ…
- 10% lower perplexity on WikiText-2 (174.19 vs 193.37)
- O(1) memory with recurrent formulation
- 16x length extrapolation (128 â†’ 2048 tokens)

---

## ğŸš€ Quick Start

```bash
# Clone
git clone https://github.com/iushv/ml_research_papers.git
cd ml_research_papers

# Navigate to a paper
cd papers/forgetting_transformer

# Setup
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Run experiments
python src/verify_claims.py           # Verify paper claims
python src/large_scale_experiments.py # WikiText-2 benchmark
```

---

## ğŸ“ Project Structure

```
arxiv_paper_implementation/
â”œâ”€â”€ papers/
â”‚   â””â”€â”€ forgetting_transformer/     # FoX implementation
â”‚       â”œâ”€â”€ src/                    # Code
â”‚       â”œâ”€â”€ results/                # Experiment data
â”‚       â”œâ”€â”€ notebooks/              # Jupyter notebooks
â”‚       â”œâ”€â”€ FINDINGS.md             # Detailed analysis
â”‚       â””â”€â”€ README.md               # Paper guide
â”œâ”€â”€ FINDINGS.md                     # Summary of all findings
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ pyproject.toml                  # Dependencies (uv)
â””â”€â”€ .venv/                          # Virtual environment
```

---

## ğŸ“– References

- [Forgetting Transformer Paper (arXiv:2503.03420)](https://arxiv.org/abs/2503.03420)

---

*Built for learning, research, and critical evaluation of ML papers.*
